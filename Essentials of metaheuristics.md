# Essentials of metaheuristics

Краткий конспект книги [https://cs.gmu.edu/~sean/book/metaheuristics/Essentials.pdf](https://cs.gmu.edu/~sean/book/metaheuristics/Essentials.pdf) с моими заметками.



## Gradient-based Optimization

### Градиентный спуск

Идём в направлении градиента. Шаг равен либо $\alpha \norm{\gradient f(\vec{x})}$, либо происходит line-search по функции в этом напрпавлении.

### Метод Ньютона

Инвартируем Гессиану, умножаем на вектор градиента, получается условно касательная. Сходится не только к минимуму, но и к максимуму и седловым точкам.



## Эволюционные алгоритмы

Часто называются двумя числами, в скобках через запятую или через плюсик: $(\alpha, \beta)$ или $(\alpha + \beta)$.

Запятая между числами означает, что лучшие отпрыски полностью замещают потомков.

Плюс - что в новую пропуляцию выбираются как дети, так и родители.

Первое число называется "$\mu$" - размер популяции, означает, сколько выживает между итерациями.

Второе - "$\lambda$" - количетво детей, вырабатывающееся на каждой итерации.

Например, $(\mu + 1)$ означает, что на каждой итерации подявляется один новый, и из родителей и его кто-то уходит.

Разумеется, не все алгоритмы так называть корректно. Например, особи могут вообще не умирать, а просто изменяться.

### One-point eolvuotion

#### Hill climbers

#### Отжиг



### Трюки для улучшения работы one-point алгоритмов

#### Перезапуск

#### Табуирование

#### Комбинация
